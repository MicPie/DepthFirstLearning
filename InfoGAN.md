# InfoGAN
Material from the lecture about InfoGAN from https://www.depthfirstlearning.com/2018/InfoGAN.

## 1 Information Theory

### Required Reading:
- [x] Chapter 1.6 from Pattern Recognition and Machine Learning / Bishop. (“PRML”)<br>
- [x] [A good intuitive explanation of Entropy, from Quora.](https://www.quora.com/What-is-an-intuitive-explanation-of-the-concept-of-entropy-in-information-theory/answer/Peter-Gribble)

### Optional Reading:
- [x] [Notes on Kullback-Leibler Divergence and Likelihood Theory](https://arxiv.org/pdf/1404.2000.pdf)
- [x] [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/) (Highly recommended!)
For more perspectives and deeper dependencies, see Metacademy:
- [ ] [Entropy](https://metacademy.org/graphs/concepts/entropy)
- [ ] [Mutual Information](https://metacademy.org/graphs/concepts/mutual_information)
- [ ] [KL diverence](https://metacademy.org/graphs/concepts/kl_divergence)

### Questions:
- [ ] From PRML:
1.31, 1.36, 1.37, 1.38, 1.39, 1.41.

- [ ] How is Mutual Information similar to correlation? How are they different? Are they directly related under some conditions?

- [ ] In classification problems, minimizing cross-entropy loss is the same as minimizing the KL divergence of the predicted class distribution from the true class distribution. Why do we minimize the KL, rather than other measures, such as L2 distance?
