**InfoGAN**
https://www.depthfirstlearning.com/2018/InfoGAN

1 Information Theory

Required Reading:

[x]Chapter 1.6 from Pattern Recognition and Machine Learning / Bishop. (“PRML”)
[x]A good intuitive explanation of Entropy, from Quora.
Optional Reading:

Notes on Kullback-Leibler Divergence and Likelihood Theory
For more perspectives and deeper dependencies, see Metacademy:
Entropy
Mutual Information
KL diverence
Visual Information Theory
Questions:

From PRML: 1.31, 1.36, 1.37, 1.38, 1.39, 1.41.
How is Mutual Information similar to correlation? How are they different? Are they directly related under some conditions?
In classification problems, minimizing cross-entropy loss is the same as minimizing the KL divergence of the predicted class distribution from the true class distribution. Why do we minimize the KL, rather than other measures, such as L2 distance?
